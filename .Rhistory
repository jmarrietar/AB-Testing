require("KernSmooth")
require(KernSmooth)
load(KernSmooth)
library(KernSmooth)
library (KernSmooth)
install.packages("KernSmooth")
library(KernSmooth)
library(data.table)
library(KernSmooth)
require("XML")
library(KernSmooth)
install.packages("KernSmooth")
library(KernSmooth
library(data.table)
library(KernSmooth)
library(caret);
install.packages("caret")
library(kernlab)
install.packages("kernlab")
data(spam)
library(kernlab)
data(spam)
View(spam)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
library(caret);
library(kernlab)
data(spam)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
inTrain
View(inTrain)
spam[inTrain,]
inTrain
spam[-inTrain,]
View(inTrain)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
dim(training)
modelFit<-train(type -.,data=training,method="glm")
modelFit<-train(type ~.,data=training,method="glm")
modelFit
modelFit<-train(type ~.,data=training,method="glm")
set.seed(32343)
modelFit<-train(type ~.,data=training,method="glm")
install.packages('e1071', dependencies=TRUE)
modelFit<-train(type ~.,data=training,method="glm")
modelFit
modelfit$finalModel
modelFit$finalModel
modelFit$finalModel
modelFit$finalModel
predictions<-predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$Type)
confusionMatrix(predictions,testing$type)
folds<-createFolds(y=spam$type,k=10,list=T,returnTrain=T)
folds
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
folds<-createResample(y=spam$type,times=10,list=T)
sapply(folds,length)
library(ISLR)
library(ggplot2)
library(caret)
library(ISLR)
library(ggplot2)
library(caret)
install.packages("ISLR")
data(Wage)
library(ISLR)
data(Wage)
data(Wage)
Wage
View(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qq+geom_smooth(method='lm',formula=y~x)
qq<-qplot(age,wage,colour=jobclass,data=training)
qq+geom_smooth(method='lm',formula=y~x)
cutWage<-cut2(training$wage,g=3)
cut2
library(ISLR)
library(ggplot2)
library(caret)
cutWage<-cut2(training$wage,g=3)
install.packages("Hmisc")
library(Hmisc)
#MAKING FACTORSS!  CUT2 Function
cutWage<-cut2(training$wage,g=3)
#MAKING FACTORSS!  CUT2 Function
cutWage<-cut2(training$wage,g=3)
cutWage
table(cutWage)
p1<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p1
t1<-table(cutWage,training$jobclass)
t1
qplot(wage,colour=education,data=training,geom="density")
library(caret);
library(kernlab)
data(spam)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve<-training$capitalAve
trainCapAveS<-(trainCapAve-mean(trainCapAve))/sd(trainCapAve)
sd(trainCapAves)
sd(trainCapAveS)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
data(AlzheimerDisease)
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
View(predictors)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
data(concrete)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(concrete)
plot(trainning$CompressiveStrength,pch=19)
plot(training$CompressiveStrength,pch=19)
plot(testing$CompressiveStrength,pch=19)
View(concrete)
plot(concrete$CompressiveStrength,pch=19)
View(concrete)
plot(concrete$FlyAsh,pch=19)
plot(concrete$FlyAsh,pch=19)
cutFlyAsh<-cut2(concrete$FlyAsh,g=4)
table(cutFlyAsh)
cutFlyAsh<-cut2(concrete$FlyAsh,g=4)
library(ISLR)
library(ggplot2)
library(caret)
library(Hmisc)
cutFlyAsh<-cut2(concrete$FlyAsh,g=4)
table(cutFlyAsh)
cutFlyAsh<-cut2(concrete$FlyAsh,g=3)
table(cutFlyAsh)
cutFlyAsh<-cut2(concrete$FlyAsh,g=3)
table(cutFlyAsh)
p1<-qplot(concrete$CompressiveStrength,data=concrete,fill=cutFlyAsh,geom=c("boxplot"))
p1
p1<-qplot(concrete$CompressiveStrength,data=concrete,fill=cutFlyAsh,geom=c("boxplot"))
p1
p1<-qplot(concrete$CompressiveStrength,data=concrete,color=cutFlyAsh,geom=c("boxplot"))
p1
p1<-qplot(concrete$CompressiveStrength,data=concrete,color=cutFlyAsh)
p1
plot(concrete$CompressiveStrength,color=cutFlyAsh)
p1<-qplot(concrete$CompressiveStrength,data=concrete)
p1
qplot(concrete$CompressiveStrength,data=concrete)
qplot(seq_along(concrete$CompressiveStrength), concrete$CompressiveStrength,data=concrete)
qplot(seq_along(concrete$CompressiveStrength), concrete$CompressiveStrength,data=concrete,colour=cutFlyAsh)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,data=training,colour=cutFlyAsh)
qplot(seq_along(concrete$CompressiveStrength), concrete$CompressiveStrength,data=concrete,colour=cutFlyAsh)
plot(training$CompressiveStrength,pch=19)
qplot(seq_along(concrete$CompressiveStrength), concrete$CompressiveStrength,data=concrete,colour=cutFlyAsh)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,data=training,colour=cutFlyAsh)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,data=training)
p1
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,data=training)
cutFlyAshT<-cut2(training$FlyAsh,g=3)
table(cutFlyAshT)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength,data=training,colour=cutFlyAshT)
qplot(seq_along(concrete$CompressiveStrength), concrete$CompressiveStrength,data=concrete,colour=cutFlyAsh)
qplot(seq_along(concrete$CompressiveStrength), concrete$FlyAsh,data=concrete,colour=cutFlyAsh)
qplot(concrete$CompressiveStrength, concrete$FlyAsh,data=concrete,colour=cutFlyAsh)
plot(concrete$Superplasticizer,pch=19)
#Question 3
hist(concrete$Superplasticizer,pch=19)
hist(log(concrete$Superplasticizer+1),pch=19)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(adData)
View(training)
TPCA<-training[,c(65.75)]
TPCA<-training[,c(65,75)]
View(TPCA)
TPCA<-training[,c(65:75)]
View(TPCA)
TPCA<-training[,c(60:75)]
View(TPCA)
TPCA<-training[,c(50:73)]
View(TPCA)
TPCA<-training[,c(55:73)]
View(TPCA)
TPCA<-training[,c(58:73)]
View(TPCA)
TPCA<-training[,c(58:69)]
View(TPCA)
TPCA<-training[,c(58:70)]
View(TPCA)
TPCA<-training[,c(58:69)]
View(TPCA)
TPCA<-training[,c(58:69),row.names = FALS]
TPCA<-training[,c(58:69),row.names = FALSE]
TPCA<-training[,c(58:69)]
TPCA$row.names<-null
TPCA$row.names<-NULL
View(TPCA)
TPCA$row.names<-NULL
View(TPCA)
preProc<-preProcess(TPCA,method="pca",pcaComp=2)
preProc<-preProcess(TPCA,method="pca",pcaComp=2)
trainPC<-predict(preProc,TPCA)
modelFit<-train(training$type~.,method="glm",data=TrainPC)
trainPC<-predict(preProc,TPCA)
modelFit<-train(training$type~.,method="glm",data=trainPC)
preProc
trainPC
preProc
preProc
trainPC
summary(trainPC)
preProc<-preProcess(TPCA,method="pca",thresh=0.8)
preProc$rotation
preProc<-preProcess(TPCA,method="pca",thresh=0.8)
preProc$rotation
#Question 4
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
TPCA<-training[,c(58:69)]
preProc<-preProcess(TPCA,method="pca",thresh=0.8)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
trainingP<-training[,c(1,58:69)]
View(trainingP)
testing
View(testing)
testing
View(testing)
testing$Type,
testing$Type
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testing$Type
testing$type
training$type
preProc
trainPC
trainPC<-predict(preProc,TPCA)
trainPC
modelFit<-train(training$type~.,method="glm",data=trainPC)
View(trainPC)
View(training)
modelFit<-train(training$diagnosis~.,method="glm",data=trainPC)
testPC<-predict(preProc,testing[,-58])
confusionMatrix(testing$diagnosis,predict(modelFit,testPC))
testPC<-predict(preProc,testing[,-58])
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainingP<-training[,c(1,58:69)]
View(trainingP)
trainPC
preProc<-preProcess(trainingP,method="pca",thresh=0.8)
preProc<-preProcess(trainingP[,-1],method="pca",thresh=0.8)
trainPC<-predict(preProc,trainingP)
trainPC<-predict(preProc,trainingP[,-1])
preProc<-preProcess(trainingP[,-1],method="pca",thresh=0.8)
trainPC<-predict(preProc,trainingP[,-1])
modelFit<-train(training$diagnosis~.,method="glm",data=trainPC)
View(testing)
testPC<-predict(preProc,testing[,-1])
trainingP<-training[,c(1,58:69)]
testingP<-testing[,c(1,58:69)]
testPC<-predict(preProc,testingP[,-1])
confusionMatrix(testing$diagnosis,predict(modelFit,testPC))
trainPC
modelFit1<-train(training$diagnosis~.,method="glm",data=training[-1])
modelFit1
confusionMatrix(testing$diagnosis,predict(modelFit1,testingP[,-1]))
confusionMatrix(testingP$diagnosis,predict(modelFit,testPC))
#one using PCA with principal components explaining 80% of the variance in the predictors.
preProc<-preProcess(trainingP[,-1],method="pca",thresh=0.8)
trainPC<-predict(preProc,trainingP[,-1])
modelFit<-train(training$diagnosis~.,method="glm",data=trainPC)
testPC<-predict(preProc,testingP[,-1])
confusionMatrix(testingP$diagnosis,predict(modelFit,testPC))
modelFit1<-train(trainingP$diagnosis~.,method="glm",data=trainingP[-1])
confusionMatrix(testingP$diagnosis,predict(modelFit1,testingP[,-1]))
Mtarix1<-confusionMatrix(testingP$diagnosis,predict(modelFit1,testingP[,-1]))
Matrix2<-confusionMatrix(testingP$diagnosis,predict(modelFit,testPC))
preProc
trainPC
preProc
Mtarix1
Matrix2
clear
#MightyHive is an advertising technology company that uses
#retargeting methods to send ads to users online.
#Call Center Remarketing, -> retarget those consumers
#online whom did not make a purchase.
setwd("C:/Users/Alfonso/Desktop/JOM/AB-Testing")
Abandoned_Data<-read.csv("Abandoned_Data_Seed.csv",header=TRUE,na.strings = "")
Reservation_Data<-read.csv("Reservation_Data_Seed.csv",header=T,na.strings = "")
###################################################################
#################DATA WRANGLING PART ##############################
###################################################################
###################################################################
########################FAILED ATEMPS##############################
###################################################################
#Important
#DIDNT WORK WITH %IN% command because NA values!!.
#I IMplemented MATCH() function insted.
#------------------ %in% function failed because of NA values in data set.
Email_matches<-Abandoned_Data$Email %in% Reservation_Data$Email
Zipcode_matches<-Abandoned_Data$Zipcode %in% Reservation_Data$Zipcode
length(Email_matches)
length(Zipcode_matches)
#--------------------------------------------------------------------
#Lets Try with Match function. --- Kind of Failed Again
table(match(Abandoned_Data$Email, Reservation_Data$Email, nomatch = 0))
table(match(Abandoned_Data$Incoming_Phone, Reservation_Data$Incoming_Phone, nomatch = 0))
sum((match(Abandoned_Data$Incoming_Phone, Reservation_Data$Incoming_Phone, nomatch = 0))>0)
Abandoned_Data$Incoming_Phone[7393]
#------------------------%In% funtion But removing NA values.
AData_Email<-Abandoned_Data$Email[!is.na(Abandoned_Data$Email)]
RData_Email<-Reservation_Data$Email[!is.na(Reservation_Data$Email)]
Email_matches<-AData_Email %in% RData_Email
sum(Email_matches>0)
#---------------------------------------------------------------
#######################################################################
########################SUCESSFULL ATEMPS##############################
#######################################################################
#THIS FINALLY WORKS. Cleaning NA and Finding Matches
#Clean NA rows with NA values in Incoming phones (cleaning) and then Use IN function.
#Remove rows with NA values in Incoming Phones column!
AData<-subset(Abandoned_Data, !is.na(Incoming_Phone))
RData<-subset(Reservation_Data, !is.na(Incoming_Phone))
Incoming_Phone_matches<-AData$Incoming_Phone %in% RData$Incoming_Phone
sum(Incoming_Phone_matches>0)
#Select only the matches from the data set
AData_Clean<-AData[Incoming_Phone_matches,]
#How many more conversions (if any) occured in the test group?
#Rta/ first determining the number of individuals whom exist in both datasets. This match indicates a caller who abandoned their
#purchase but then came back and made a reservation.
#If an Abandoned client made  a purchase then his mail will appear in the reservation data.
#I create matches between emails.
#Data cleaning:
#The effectiveness of MightyHive's retargeting product is
#qualified only by the conversion of a customer the first time and not any additional reservations.
#WE NEED TO REMOVE DUPLICATE in Reservation Data.
# Original data with repeats removed. These do the same:
#Select only the matches from the data set
AData_Clean<-AData[Incoming_Phone_matches,]
#Now, We need to remove duplicated values from the Data base which the matches.
AData_Clean_Unique<-AData_Clean[!duplicated(AData_Clean$Incoming_Phone),]
AData_Unique<-unique(AData)
###################################################################
##################### AB TESTING PART #############################
###################################################################
#Every caller that abandoned their phone call to the call center
#was randomly split into test and control groups with an approximate 50/50 split.
#The abandoned observations in the test group were then
#re-targeted with advertisements online for a period of 7 days.
#The control group was never shown any advertisements.
#I want to see if the Reservation rate for test group is higher than for control group.
#Difference between Reservation rates between test group and control group.
#If that is the case the the retargeting is sucessfull.
#Null hypothesis: the difference of Reservation rates between test group and control is equal to 0
#Alternative hypothesis: the difference of Reservation rates between test group and control is greater than 0.
#Sample size>30 the Z test.
#The t.test() function in R can quickly perform the statistical analysis and recognizes the greater than 30 sample size, thereby using the normal distribution.
#Count the number of matches with the label 'test' and 'control' respectively. (From Unique and Clean data base  of matches)
test_obs<-sum(AData_Clean_Unique[,12]=="test")
control_Obs<-sum(AData_Clean_Unique[,12]=="control")
##Count the number of rows the label 'test' and 'control' respectively from Unique original Abandoned data.
AData_total_test<-sum(AData_Unique[,12]=="test")
AData_total_control<-sum(AData_Unique[,12]=="control")
#Get proportion of the values above.
test_proportion<-test_obs/AData_total_test
control_proportion<-control_Obs/AData_total_control
#Get Z statistic
#What are the ods of getting this statistic again.
test_statistic<-test_proportion-control_proportion
#Pooled proportion
pooled_proportion<-(sum(test_obs)+sum(control_Obs))/(AData_total_test+AData_total_control)
#Standard error<- Variability of oru test statistic inits sampling distribution.
#This is the denominator or ecuation.
denominator=sqrt(pooled_proportion*(1-pooled_proportion)*((1/AData_total_test)+(1/AData_total_control)))
SE=denominator
Z=test_statistic/denominator
#Z-score Is:
#How many standard deviations our observed test statistics
#calculates assuming the null hypothesis is true.
#Pvalue
#pnorm calculates the area under the normal curve to the right of our Z score.
p_value<-pnorm(Z,lower.tail=F)
p_value
#Confidence intervals.
upper_bound <- test_proportion - control_proportion + 1.96*SE
lower_bound <- test_proportion - control_proportion - 1.96*SE
test_statistic
test_obs
control_Obs
test_obs
AData_total_test
Delivery_Data<-read.csv("Delivery_Data_problem.csv",header=T)
View(Delivery_Data)
table(Delivery_Data$pickup_name)
sort(table(Delivery_Data$pickup_name))
Delivery_Data[Delivery_Data$pickup_name=="Safeway"]
Delivery_Data[,Delivery_Data$pickup_name=="Safeway"]
colnames(Delivery_Data)
Delivery_Data[,18]
Delivery_Data[,18]=="Safeway"
DeliveryData[Delivery_Data[,18]=="Safeway"]
Delivery_Data[Delivery_Data[,18]=="Safeway"]
Delivery_Data[,Delivery_Data[,18]=="Safeway"]
Delivery_Data[Delivery_Data[,18]=="Safeway"]
Delivery_Data(Delivery_Data[,18]=="Safeway")
Delivery_Data[Delivery_Data[,18]=="Safeway",]
Delivery_Data[Delivery_Data[,18]=="Safeway",][1]
Delivery_Data[Delivery_Data[,18]=="Safeway",][,1]
Delivery_Data[Delivery_Data[,18]=="Safeway",][1,]
Delivery_Data[Delivery_Data[,18]=="Safeway",][1,]
Delivery_Data[Delivery_Data[,18]=="Safeway",]
colnames(Delivery_Data)
colnames(Delivery_Data)
Delivery_Data[Delivery_Data[,18]=="Safeway",][,7]
sort(table(Delivery_Data[Delivery_Data[,18]=="Safeway",][,7]))
colnames(Delivery_Data)
sort(table(Delivery_Data[Delivery_Data[,18]=="Safeway",][,14]))
sort(table(Delivery_Data$pickup_zipcode))
sort(table(Delivery_Data$dropoff_zipcode))
(Delivery_Data[Delivery_Data[,15]==5,])
(Delivery_Data[Delivery_Data[,15]=="5",])
(Delivery_Data[Delivery_Data[,15]=="5",])
(Delivery_Data[Delivery_Data[,15]=="5",])[,15]
colnames(Delivery_Data)
(Delivery_Data[Delivery_Data[,15]=="5",])[,12]
mean((Delivery_Data[Delivery_Data[,15]=="5",])[,12])
mean((Delivery_Data[Delivery_Data[,15]=="4",])[,12])
mean((Delivery_Data[Delivery_Data[,15]=="3",])[,12])
mean((Delivery_Data[Delivery_Data[,15]=="2",])[,12])
mean((Delivery_Data[Delivery_Data[,15]=="1",])[,12])
colnames(Delivery_Data)
mean((Delivery_Data[Delivery_Data[,13]=="1",])[,12])
mean((Delivery_Data[Delivery_Data[,13]=="1",])[,12])/60
mean((Delivery_Data[Delivery_Data[,13]=="0",])[,12])/60-mean((Delivery_Data[Delivery_Data[,13]=="1",])[,12])/60
mean((Delivery_Data[Delivery_Data[,13]=="0",])[,12])/60
mean((Delivery_Data[Delivery_Data[,13]=="1",])[,12])/60
mean((Delivery_Data[Delivery_Data[,13]==1,])[,12]) - mean((Delivery_Data[Delivery_Data[,13]==0,])[,12])/60
mean((Delivery_Data[Delivery_Data[,13]==1,])[,12]) - mean((Delivery_Data[Delivery_Data[,13]==0,])[,12])
mean((Delivery_Data[Delivery_Data[,13]>0,])[,12]) - mean((Delivery_Data[Delivery_Data[,13]==0,])[,12])
(mean((Delivery_Data[Delivery_Data[,13]>0,])[,12]) - mean((Delivery_Data[Delivery_Data[,13]==0,])[,12]))/60
colnames(Delivery_Data)
(Delivery_Data[Delivery_Data[,13]>0,])
colnames(Delivery_Data)
(Delivery_Data[Delivery_Data[,13]>0,])[,15]
mean((Delivery_Data[Delivery_Data[,13]>0,])[,15])
mean((Delivery_Data[Delivery_Data[,13]>0,])[,15],na.rm=T)
help mean
help(mean)
mean((Delivery_Data[Delivery_Data[,13]>0,])[,15],na.rm=F)
mean((Delivery_Data[Delivery_Data[,13]>0,])[,15],na.rm=TRUE)
mean(as.integer((Delivery_Data[Delivery_Data[,13]>0,])[,15]),na.rm=TRUE)
mean(as.integer((Delivery_Data[Delivery_Data[,13]>0,])[,15]),na.rm=TRUE)
colnames(Delivery_Data)
mean(as.integer((Delivery_Data[Delivery_Data[,13]>0,])[,15]))
